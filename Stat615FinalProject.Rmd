---
title: "Stat615FinalProject"
author: "Dhruv Jain & mekdim"
date: "2023-04-20"
output:
  html_document: default
  word_document: default
---

```{r,warning=FALSE,message=FALSE }
# calling all the libraries used in the code book 
library(olsrr)
library(tidyverse)
library(dbplyr)
library(dplyr)
library(Matrix)
library(MASS)
library(ggplot2)
library(tibble)
library(data.table)
library(ggmosaic)
library(ggforce)
library(ggmap)
library(ggthemes)
library(purrr)
library(keep)
library(readr)
library(gridExtra)
library(randomForest)

```

# 1 

## 1.1 About Data set 
```{r}
# offer a preliminary description of the data set. For example, indicate the size of the data source, describe the variables, and include any other data profile information that would be of interest.

#data set source: https://www.kaggle.com/datasets/mirichoi0218/insurance

# Columns Description

#age: age of primary beneficiary
#sex: insurance contractor gender, female, male
#bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,
#objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
#children: Number of children covered by health insurance / Number of dependents
#smoker: Smoking
#region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
#charges: Individual medical costs billed by health insurance


# We had to randomly sample 300 rows from our original data. 
# We then saved these 300 rows into csv file so that we can import them later.
# going forward We will take that csv file. (which has only be run once)

# The preliminary steps we did 
#insurance <- read_csv('Downloads/insurance.csv')
#insurance_300 <- sample_n(insurance, 300)

#write.csv(insurance_300 , file = "Desktop/insurance_300.csv")

# Let import the 300 rows 

# insurance_new <- read_csv("insurance_300.csv")

# Read in CSV file and specify column types
insurance_new <- read_csv("insurance_300.csv", 
col_types = cols(
  age = col_double(),
  sex = col_character(),
  bmi = col_double(),
  children = col_double(),
  smoker = col_character(),
  region = col_character(),
  charges = col_double()
))
#300 rows  and 7 columns 
# This project is about determining the factors that affect medical costs billed by health insurance 
# The independent variables include three categorical variables and three quantitative variables. 
# Sex, region(Northeast, northwest etc), and smoker(whether a person smokes or not) are the categorical 
# variables. While the quantitative variables include the BMI index, the age and the number of children the person have. 

nrow(insurance_new)
ncol(insurance_new)

# Let us quickly investigate the summary of our dependent variable 
#  The median insurance charge is around 10097 and the mean of 13283. The 
# standard deviation is 11399.

summary(insurance_new$charges)
sd(insurance_new$charges)
head(insurance_new,10)
```

## 1.2  cleaning the data and type of columns 

```{r}
# calling the data set using read csv file
# insurance_new <- read_csv('insurance_300.csv')
# number of rows in data 
nrow(insurance_new)
# number of colums in data set 
ncol(insurance_new)
# colums names 
colnames(insurance_new)
# visual data set look like 
head(insurance_new,10)
# type of columns used in data frame (double, charterer)
str(insurance_new)

# summary of data colum wise 
summary(insurance_new)
# calculating NA/missing data in columns 
colSums(is.na(insurance_new))

# converting to factor variable 
insurance_new$sex = as.factor(insurance_new$sex)
insurance_new$smoker = as.factor(insurance_new$smoker)
# how many unique values 
unique(insurance_new$sex)
unique(insurance_new$children)
unique(insurance_new$smoker)
unique(insurance_new$region)
```

## 1.3 visualization

```{r}

# Does age affect medical charges for smoker? 
ggplot(data = insurance_new , 
       aes(x=age, y=charges,shape=smoker,color = smoker)) +
  geom_point()+
  geom_smooth(method=lm)
# Yes the charges are increased as we increase the number of age. Now the fun part is if a person smokes he/she is paying more charges on medical then the person not smoking.

# Does Body mass index (BMI) affect medical charges for smoker? 
ggplot(data = insurance_new , 
       aes(x=bmi, y=charges,shape=smoker,color = smoker)) +
  geom_point()+
  geom_smooth(method=lm)
# One can clearly observe that smoking affect in BMI and incresased  with the medical expenses. 



ggplot(data = insurance_new , 
       aes(x=children, y=charges,shape=smoker))+
  geom_point()+
  stat_ellipse()





# Who smoke a lot male or female ? 
# does no. of children affect the score?
ggplot(data = insurance_new , 
       aes(x=children, y=smoker, shape=sex, color=sex)) +
  geom_point()




insurance_new%>%
  filter(sex == "female")%>%
  count(sex,children,smoker,region)%>%
  arrange(sex, smoker)
  


insurance_new  
ab <- insurance_new%>%
  dplyr::select(age,smoker,sex)
ab

#plot()
```



# 2  multicollinearity

```{r}
# Let do correlation matrix with the numeric variables first to see and inspect
#  As we can from the correlation  matrix, there seems no multi-colinearity. 
# The highest correlation is between charges and age with only 0.24. But if we exclude charges
# since charges is dependent variable, the highest correlation among the independent variables 
# is age with bmi with only 0.04 which is essentially 0. So there exists no 
# colinearity among the independent variables. This suggests that each of the variables might be
# useful if they are included in the regression model as they dont have any correlation with each other. 

numeric_insurance <- insurance_new[, c("bmi", "children", "age", "charges")]
#cor(numeric_insurance)

# we can also the scatter plots between the independent variables 
# clearly there is no pattern that we can see verifying our output from the 
# correlation matrix. 
pairs(numeric_insurance[,1:3], pch = 19, lower.panel = NULL)
#
```

Matrix Method with just the quantitative variables 
```{r}

# Both the matrix and lm method produced the same coefficients . 
lm(charges ~ age + children + bmi, data= insurance_new)

X <- model.matrix(~age + children + bmi   , data=insurance_new )

Y <- as.matrix(insurance_new%>%dplyr::select(charges) )

Xm <- X
Ym <- Y

t(Xm) -> transposeXm
transposeXm%*%Xm-> ProDuct1
solve(ProDuct1)%*%transposeXm%*%Ym -> interceptandslope
interceptandslope

```
Fitted and residual values from the matrix result 

```{r}
Xm %*% interceptandslope -> fitted_values  

Ym -  Xm %*% interceptandslope -> residuals_values

data.frame(residuals_values, fitted_values)
```
Matrix method with both quantitative and qualitative variables(dummy variables included automatically)
```{r}

# The results are the same using both the matrix method and lm method. 


X <- model.matrix(~age + children + bmi  + region  + sex + smoker , data=insurance_new )

Y <- as.matrix(insurance_new%>%dplyr::select(charges) )

Xm <- X
Ym <- Y

t(Xm) -> transposeXm
transposeXm%*%Xm-> ProDuct1
solve(ProDuct1)%*%transposeXm%*%Ym -> interceptandslope
interceptandslope
#lm(charges ~ age + children + bmi + region + sex + smoker,insurance_new) -> x

#summary(x)

```

lm method including both categorical and quantitative variables 

```{r}
lm(charges ~ age + children + bmi + region + sex + smoker,insurance_new) -> x

summary(x)
```
 Evaluating Various regression models using summary tables and statistics 
```{r}

#  Let us begin by using a multiple linear regression model that uses all the six variables. 
#  From the summary table we see that our R squared and Adjusted r square are around 0.73 and the
# residual standard error is 5915. 
#  The r squared value is high enough to be considered good but let us continue finding better fits. 

lm(charges ~ age + children + bmi + region + sex + smoker,insurance_new) -> x
summary(x)



# Let us investigate the levels of the catagorical variables 
# There are two levels female and male 
levels(factor(insurance_new$sex) )
# Four regions : northeast northwest , southeast, southwest 
levels( factor( insurance_new$region  ) )
# Two levels - Smoker or not smoker 
levels ( factor(insurance_new$smoker))

# Let us see how r will create dummy variables for us using Region variable. 
# R will convert the categorical variables for us 
# We can see R will do this automatically for us creating these dummy variables 
# northeast is essentially the control variable. When the three variables are 0,
# it means that region is northeast! 
# No need to bother here as r does this for us
contrasts(as.factor(insurance_new$region))






# Next step - Let us include all the interaction terms as well. Our residual standard error reduced 
# significantly to 4910. Our R squared also increased to 0.8331. 

lm(charges~ age + children + bmi + 
     region + sex + smoker + age:children + age:bmi + age:region + age:sex + age:smoker+
     children:bmi + children:region + children:sex + children:smoker
     + bmi:region + bmi:sex+ bmi:smoker + region:sex + region + smoker
   + sex:smoker, insurance_new) -> interactionModel
summary(interactionModel)


# Let us also see if transforming our dependent variable might help. Our R squared increased slightly
# but not much

lm(log(charges)~ age + children + bmi + 
     region + sex + smoker + age:children + age:bmi + age:region + age:sex + age:smoker+
     children:bmi + children:region + children:sex + children:smoker
   + bmi:region + bmi:sex+ bmi:smoker + region:sex + region + smoker
   + sex:smoker, insurance_new) -> interactionModel
summary(interactionModel)

# Let us add more interaction terms that include transformed x variables 
# our y variable has been transformed here as well. Our r squared increased again to
# 0.8373 and the residual error is now 4839. It is better slightly but there are a lot 
# of variables which are not significant. For example, sex:smoke interaction variable's
# p value is 0.7829 which is significanly above 0.05.
# In the next step, let us remove all those variables that are not significant 

lm(charges~ age + children + bmi + 
     region + sex + smoker + age:children + age:bmi + age:region + age:sex + age:smoker+
      children:sex + children:smoker
   + bmi:region + bmi:sex+ bmi:smoker + region:sex + region + smoker
   + sex:smoker + log(bmi)+ bmi*bmi + log(age) + age*age + log(age)*log(bmi), insurance_new) -> interactionModel
summary(interactionModel)

```
Producing a reduced model (removing variables of our choice with justification)

```{r}
# #  Let eliminate all interaction 
# terms or the single whose p value is insignificant. For this case our starting model has a lot 
# of variables so after removing many of the variables we will have a reduced model.
# Those variables whose p value is too high should be removed and a reduced model has to be produced. 
# we run the reduced model below
# our r squared slightly decreased to 0.8332 and the adjusted r squared to 0.8244. 
# It is very small change to our previous step so it is fine to take this. The 
# p values are also significant so for that reason we chose this model. This could be one candidate model
# for our regression. 


lm(charges~  children + bmi + 
      smoker + age:children + age:region + age:smoker+
       + children:smoker + bmi:region +  bmi:smoker + smoker
   +  log(bmi)+ bmi*bmi + log(age)  , insurance_new) -> interactionModelSignificant
summary(interactionModelSignificant)

# let also use the variables from the last step to estabilish model selection like we have covered in class
# R will use any combination of these variables to come up with r^2, cp values etc for each of them. 
# we will order the result by r^2, then by adjusted then by cp and inspect if we should choose any other combination of the 
# variables from above. 

# not running it for now - slow
#k <- ols_step_all_possible(interactionModel)
#as_tibble(k) -> tk
#arr <-arrange(tk, -rsquare, -cp, -adjr)
# As we can see the maximum r square values are around 0.833 (from 2047 models ) which aligns with our finding from above. So need to change anything.
# Since there are a lot of models whose r squared is 0.833 we might choose the one with the lowest cp. 
# And that is the 6th row with cp of 5.16. 
# the variables used are 
# "smoker log(bmi) log(age) age:region smoker:age children:smoker bmi:region bmi:smoker".  with 8 variables. 
# so that could be an alternative reduced version of the model we have above. 

#arr
#arr[['predictors']][6:6]



# So going forward let me  choose the model from above. We felt it was good enough so now let us investigate more
# by analysing the residuals and the normality. 
lm(charges~  children + bmi + 
     smoker + age:children + age:region + age:smoker+
     + children:smoker + bmi:region +  bmi:smoker + smoker
   +  log(bmi)+ bmi*bmi + log(age)  , insurance_new) -> interactionModelSignificant

# The residual plot is not perfect but there is no clear pattern. So it should be relatively fine. 
# It indicates relatively good amount of constant variance 

resid(interactionModelSignificant) -> residuals1
plot(fitted(interactionModelSignificant), residuals1)
abline(0,0)
```

Showing confidence intervals for  two of our chosen quantitative variables 

```{r}
#using this regression model Let us take at least two variables and find confidence interval
# for the independent variables.  Let us take two such as  log(age) and bmi for example. 

# The standard error for log(age) is 1406.95 and the coefficient is 5125.69. 
# and the standard error for bmi is 454 and the coeffient is 1285.83 . 

# so to find the confidence interval, let us find the t statistics 

#  degree of freedom is 284. (300-16). t is 1.9683.  Which is closer to the z score actually.
qt(p=.025, df=284, lower.tail = FALSE) -> t
t

# so for bmi 
#upper bound    2179.438
1285.83 + 1.9683 * 454
# lower bound   392.2218
1285.83 - 1.9683 * 454

# for log(age)
# upper bound ] 7894.99 
5125.69 + 1.9683 * 1406.95  
# lower bound 
5125.69 - 1.9683 * 1406.95 



```

Researching and Applying a model analysis not discussed in the class
```{r}
# Here we will show how random forest model can be used on our full model or using 
# all explanatory variables to predict the dependent variable charges. 
# Random forest works by using if-else decision trees using all variables.
# For example one possible path could be if a person is a non smoker, male and if he has
# a bmi value above 90, the medical charge should approximately be 1000. This is just to 
# show how it works under the hood but the actual mechanisms and branching rules are not as
# trivial as  my example. For a continuous dependent variable, we should use random forest regressor
# (it does regression but using random forest)
# We will compare the root mean squared error using our random forest model and the regression
# we had above 
# 
# 
# Random Forest Model. 
#install.packages("randomForest")
set.seed(42)
rf.fit <- randomForest(charges ~ ., data=insurance_new, ntree=3,
                    keep.forest=FALSE, importance=TRUE)

# from fitting the random forest model, we can see that the root mean squared is 37712690^(0.5). Rf.fit gives
# us the squared value so we have to take the root of it to find the root mean square. so
# 37712690^(0.5) = 6141.066. This random forest model produced worse result than our regression model. 
# we got a value of 4858 as our best root mean squared error from our regressoin model.
rf.fit

# But also just like we can tune our regression model, we can also tune our random forest model.
# Let us increase the number of trees from 3 to 300 in the random forest model. 
# we get 25902511^(0.5) = 5089.451. So as we increase our number of trees the root mean squared 
# approached our best regression model out put. Of course we can tune a lot of things
# in the decision trees of random forest as well so random might give us a root mean square value less than
# our regression model. 
# The variability explained by this random forest model was also close to what we have in the regression model.
# It is 80% here. 

rf.fit <- randomForest(charges ~ ., data=insurance_new, ntree=300,
                       keep.forest=FALSE, importance=TRUE)
rf.fit


# Let also this which variables were important according to the latest random forest model
# we had. We see that smoker variable was very important in terms of information gain( it is one of the
#best variable used in the decision tree and is found to be important interms of determing 
# medical charges/bills. The other variables that are found important are age and BMI as we see from the diagram.)

# children, region and sex had minimal impact compared to the other variables. 

ImpData <- as.data.frame(importance(rf.fit))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )


```

---
title: "Stat615FinalProject"
output: html_document
date: "2023-04-20"
---

```{r }
# 
library('tidyverse')
library('dplyr')
library(olsrr)

# We had to randomly sample 300 rows from our original data. 
# We then saved these 300 rows into csv file so that we can import them later.
# going forward We will take that csv file. (which has only be run once)

# The preliminary steps we did 
#insurance <- read_csv('Downloads/insurance.csv')
#insurance_300 <- sample_n(insurance, 300)

#write.csv(insurance_300 , file = "Desktop/insurance_300.csv")

# Let import the 300 rows 
insurance_new <- read_csv('insurance_300.csv')
#300 rows  and 7 columns 
# This project is about determining the factors that affect medical costs billed by health insurance 
# The independent variables include three categorical variables and three quantitative variables. 
# Sex, region(Northeast, northwest etc), and smoker(whether a person smokes or not) are the categorical 
# variables. While the quantitative variables include the BMI index, the age and the number of children the person have. 

nrow(insurance_new)
ncol(insurance_new)
# Let us quickly investigate the summary of our dependent variable 
#  The median insurance charge is around 10097 and the mean of 13283. The 
# standard deviation is 11399. 
summary(insurance_new$charges)
sd(insurance_new$charges)

# Let do correlation matrix with the numeric variables first to see and inspect
# multicoliinearity 
#  As we can from the correlation  matrix, there seems no multi-colinearity. 
# The highest correlation is between charges and age with only 0.24. But if we exclude charges
# since charges is dependent variable, the highest correlation among the independent variables 
# is age with bmi with only 0.04 which is essentially 0. So there exists no 
# colinearity among the independent variables. This suggests that each of the variables might be
# useful if they are included in the regression model as they dont have any correlation with each other. 

numeric_insurance <- insurance_new[, c("bmi", "children", "age", "charges")]
cor(numeric_insurance)
# we can also the scatter plots between the independent variables 
# clearly there is no pattern that we can see verifying our output from the 
# correlation matrix. 
pairs(numeric_insurance[,1:3], pch = 19, lower.panel = NULL)

#  Let us begin by using a multiple linear regression model that uses all the six variables. 
#  From the summary table we see that our R squared and Adjusted r square are around 0.73 and the
# residual standard error is 5915. 
#  The r squared value is high enough to be considered good but let us continue finding better fits. 

lm(charges ~ age + children + bmi + region + sex + smoker ,  
   insurance_new) -> x
summary(x)



# Let us investigate the levels of the catagorical variables 
# There are two levels female and male 
levels(factor(insurance_new$sex) )
# Four regions : northeast northwest , southeast, southwest 
levels( factor( insurance_new$region  ) )
# Two levels - Smoker or not smoker 
levels ( factor(insurance_new$smoker))

# Let us see how r will create dummy variables for us using Region variable. 
# R will convert the categorical variables for us 
# We can see R will do this automatically for us creating these dummy variables 
# northeast is essentially the control variable. When the three variables are 0,
# it means that region is northeast! 
# No need to bother here as r does this for us
contrasts(as.factor(insurance_new$region))






# Next step - Let us include all the interaction terms as well. Our residual standard error reduced 
# significantly to 4910. Our R squared also increased to 0.8331. 

lm(charges~ age + children + bmi + 
     region + sex + smoker + age:children + age:bmi + age:region + age:sex + age:smoker+
     children:bmi + children:region + children:sex + children:smoker
     + bmi:region + bmi:sex+ bmi:smoker + region:sex + region + smoker
   + sex:smoker, insurance_new) -> interactionModel
summary(interactionModel)


# Let us also see if transforming our dependent variable might help. Our R squared increased slightly
# but not much

lm(log(charges)~ age + children + bmi + 
     region + sex + smoker + age:children + age:bmi + age:region + age:sex + age:smoker+
     children:bmi + children:region + children:sex + children:smoker
   + bmi:region + bmi:sex+ bmi:smoker + region:sex + region + smoker
   + sex:smoker, insurance_new) -> interactionModel
summary(interactionModel)

# Let us add more interaction terms that include transformed x variables 
# our y variable has been transformed here as well. Our r squared increased again to
# 0.8373 and the residual error is now 4839. It is better slightly but there are a lot 
# of variables which are not significant. For example, sex:smoke interaction variable's
# p value is 0.7829 which is significanly above 0.05.
# In the next step, let us remove all those variables that are not significant 

lm(charges~ age + children + bmi + 
     region + sex + smoker + age:children + age:bmi + age:region + age:sex + age:smoker+
      children:sex + children:smoker
   + bmi:region + bmi:sex+ bmi:smoker + region:sex + region + smoker
   + sex:smoker + log(bmi)+ bmi*bmi + log(age) + age*age + log(age)*log(bmi), insurance_new) -> interactionModel
summary(interactionModel)

# #  Let eliminate all interaction 
# terms whose p value is insignificant
# our r squared slightly decreased to 0.8332 and the adjusted r squared to 0.8244. 
# It is very small change to our previous step so it is fine to take this. The 
# p values are also significant so this could be one candidate model
# for our regression. 

lm(charges~  children + bmi + 
      smoker + age:children + age:region + age:smoker+
       + children:smoker + bmi:region +  bmi:smoker + smoker
   +  log(bmi)+ bmi*bmi + log(age)  , insurance_new) -> interactionModelSignificant
summary(interactionModelSignificant)

# let also use the variables from the last step to estabilish model selection like we have covered in class
# R will use any combination of these variables to come up with r^2, cp values etc for each of them. 
# we will order the result by r^2, then by adjusted then by cp and inspect if we should choose any other combination of the 
# variables from above. 

# not running it for now - slow
#k <- ols_step_all_possible(interactionModel)
#as_tibble(k) -> tk
#arr <-arrange(tk, -rsquare, -cp, -adjr)
# As we can see the maximum r square values are around 0.833 (from 2047 models ) which aligns with our finding from above. So need to change anything.
# Since there are a lot of models whose r squared is 0.833 we might choose the one with the lowest cp. 
# And that is the 6th row with cp of 5.16. 
# the variables used are 
# "smoker log(bmi) log(age) age:region smoker:age children:smoker bmi:region bmi:smoker".  with 8 variables. 
# so that could be an alternative reduced version of the model we have above. 

#arr
#arr[['predictors']][6:6]



# So going forward let me  choose the model from above. We felt it was good enough so now let us investigate more
# by analysing the residuals and the normality. 
lm(charges~  children + bmi + 
     smoker + age:children + age:region + age:smoker+
     + children:smoker + bmi:region +  bmi:smoker + smoker
   +  log(bmi)+ bmi*bmi + log(age)  , insurance_new) -> interactionModelSignificant

# The residual plot is not perfect but there is no clear pattern. So it should be relatively fine. 
# It indicates relatively good amount of constant variance 

resid(interactionModelSignificant) -> residuals1
plot(fitted(interactionModelSignificant), residuals1)
abline(0,0)


#using this regression model Let us take at least two variables and find confidence interval
# for the independent variables.  Let us take two such as  log(age) and bmi for example. 

# The standard error for log(age) is 1406.95 and the coefficient is 5125.69. 
# and the standard error for bmi is 454 and the coeffient is 1285.83 . 

# so to find the confidence interval, let us find the t statistics 

#  degree of freedom is 284. (300-16). t is 1.9683.  Which is closer to the z score actually.
qt(p=.025, df=284, lower.tail = FALSE) -> t
t

# so for bmi 
#upper bound    2179.438
1285.83 + 1.9683 * 454
# lower bound   392.2218
1285.83 - 1.9683 * 454

# for log(age)
# upper bound ] 7894.99 
5125.69 + 1.9683 * 1406.95  
# lower bound 
5125.69 - 1.9683 * 1406.95 



```

